{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mario with Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import random, datetime, os, copy\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 7\n"
     ]
    }
   ],
   "source": [
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Action :\",n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240, 256, 3), {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gym is worked or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# done = True\n",
    "# for step in range(500):\n",
    "#     if done:\n",
    "#         next_state = env.reset()\n",
    "#     next_state, reward, done,_, info = env.step(env.action_space.sample())\n",
    "#     env.render()\n",
    "#     #img=env.render(mode=\"rgb_array\")\n",
    "    \n",
    "# env.close()\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grey Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v1 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((240, 256, 3), {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v1\", render_mode='human', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Action :\",n_actions)\n",
    "\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 84, 84), {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "# env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "# env.seed(42)\n",
    "# env.action_space.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "# torch.random.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNSolver(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        ).to(device) \n",
    "        self.target = copy.deepcopy(self.online)\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, action_dim, save_directory):\n",
    "        self.action_dim = action_dim\n",
    "        self.save_directory = save_directory\n",
    "        self.net = DDQNSolver(self.action_dim) #.cuda()\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_rate_decay = 0.99\n",
    "        self.exploration_rate_min = 0.01\n",
    "        self.current_step = 0\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.sync_period = 1e4\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025, eps=1e-4)\n",
    "        self.loss = torch.nn.SmoothL1Loss()\n",
    "        self.episode_rewards = []\n",
    "        self.moving_average_episode_rewards = []\n",
    "        self.current_episode_reward = 0.0\n",
    "\n",
    "    def log_episode(self):\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "        self.current_episode_reward = 0.0\n",
    "\n",
    "    def log_period(self, episode, epsilon, step):\n",
    "        self.moving_average_episode_rewards.append(np.round(np.mean(self.episode_rewards[-checkpoint_period:]), 3))\n",
    "        print(f\"Episode {episode} - Step {step} - Epsilon {epsilon} - Mean Reward {self.moving_average_episode_rewards[-1]}\")\n",
    "        plt.plot(self.moving_average_episode_rewards)\n",
    "        plt.savefig(os.path.join(self.save_directory, f\"episode_rewards_plot_{episode}.png\"))\n",
    "        plt.clf()\n",
    "\n",
    "    def remember(self, state, next_state, action, reward, done):\n",
    "        self.memory.append((torch.tensor(state.__array__()), torch.tensor(next_state.__array__()),\n",
    "                            torch.tensor([action]), torch.tensor([reward]), torch.tensor([done])))\n",
    "\n",
    "    def experience_replay(self, step_reward):\n",
    "        self.current_episode_reward += step_reward\n",
    "        if self.current_step % self.sync_period == 0:\n",
    "            self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "        if self.batch_size > len(self.memory):\n",
    "            return\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "        q_estimate = self.net(state.to(device), model=\"online\")[np.arange(0, self.batch_size), action.to(device)]\n",
    "        with torch.no_grad():\n",
    "            best_action = torch.argmax(self.net(next_state.to(device), model=\"online\"), dim=1)\n",
    "            next_q = self.net(next_state.to(device), model=\"target\")[np.arange(0, self.batch_size), best_action]\n",
    "            q_target = (reward.to(device) + (1 - done.to(device).float()) * self.gamma * next_q).float()\n",
    "        loss = self.loss(q_estimate, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def recall(self):\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*random.sample(self.memory, self.batch_size)))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            action_values = self.net(torch.tensor(state.__array__()).unsqueeze(0).to(device), model=\"online\") #.cuda()\n",
    "            action = torch.argmax(action_values, dim=1).item()\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        self.current_step += 1\n",
    "        return action\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.net.load_state_dict(checkpoint['model'])\n",
    "        self.exploration_rate = checkpoint['exploration_rate']\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        filename = os.path.join(self.save_directory, 'checkpoint_{}.pth'.format(episode))\n",
    "        torch.save(dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_period = 10\n",
    "save_directory = \"mario_ql\"\n",
    "load_checkpoint = 'checkpoint_320.pth'\n",
    "agent = DDQNAgent(action_dim=env.action_space.n, save_directory=save_directory)\n",
    "if load_checkpoint is not None:\n",
    "    agent.load_checkpoint(save_directory + \"/\" + load_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Episode 330 - Step 1498 - Epsilon 0.01 - Mean Reward 764.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_330.pth'\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Episode 340 - Step 2836 - Epsilon 0.01 - Mean Reward 746.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_340.pth'\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Episode 350 - Step 4721 - Epsilon 0.01 - Mean Reward 735.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_350.pth'\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Episode 360 - Step 6344 - Epsilon 0.01 - Mean Reward 757.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_360.pth'\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Episode 370 - Step 7109 - Epsilon 0.01 - Mean Reward 560.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_370.pth'\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Episode 380 - Step 8673 - Epsilon 0.01 - Mean Reward 820.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_380.pth'\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Episode 390 - Step 9759 - Epsilon 0.01 - Mean Reward 768.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_390.pth'\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Episode 400 - Step 11085 - Epsilon 0.01 - Mean Reward 765.4\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_400.pth'\n",
      "Clear Stage\n",
      "Clear Stage\n",
      "Clear Stage\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ddqn.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m next_state, reward, done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m agent\u001b[39m.\u001b[39mremember(state, next_state, action, reward, done)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m agent\u001b[39m.\u001b[39;49mexperience_replay(reward)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ddqn.ipynb Cell 18\u001b[0m in \u001b[0;36mDDQNAgent.experience_replay\u001b[1;34m(self, step_reward)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m q_estimate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(state\u001b[39m.\u001b[39mto(device), model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39monline\u001b[39m\u001b[39m\"\u001b[39m)[np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size), action\u001b[39m.\u001b[39mto(device)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     best_action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(next_state\u001b[39m.\u001b[39;49mto(device), model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39monline\u001b[39;49m\u001b[39m\"\u001b[39;49m), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     next_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(next_state\u001b[39m.\u001b[39mto(device), model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m)[np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size), best_action]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     q_target \u001b[39m=\u001b[39m (reward\u001b[39m.\u001b[39mto(device) \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m done\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_q)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ddqn.ipynb Cell 18\u001b[0m in \u001b[0;36mDDQNSolver.forward\u001b[1;34m(self, input, model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, model):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39monline\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monline(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39melif\u001b[39;00m model \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget(\u001b[39minput\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "episode = 320\n",
    "while True:\n",
    "    state,info = env.reset()\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        env.render()\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        agent.remember(state, next_state, action, reward, done)\n",
    "        agent.experience_replay(reward)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print('Clear Stage')\n",
    "            episode += 1\n",
    "            agent.log_episode()\n",
    "            if episode % checkpoint_period == 0:\n",
    "                agent.log_period(episode=episode, epsilon=agent.exploration_rate, step=agent.current_step)\n",
    "                agent.save_checkpoint()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
