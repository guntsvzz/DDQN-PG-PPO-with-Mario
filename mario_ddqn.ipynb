{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mario with Double Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import random, datetime, os, copy\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v1 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 7\n"
     ]
    }
   ],
   "source": [
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v1\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Action :\",n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240, 256, 3), {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gym is worked or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# done = True\n",
    "# for step in range(500):\n",
    "#     if done:\n",
    "#         next_state = env.reset()\n",
    "#     next_state, reward, done,_, info = env.step(env.action_space.sample())\n",
    "#     env.render()\n",
    "#     #img=env.render(mode=\"rgb_array\")\n",
    "    \n",
    "# env.close()\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grey Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((240, 256, 3), {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v1\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Action :\",n_actions)\n",
    "\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 84, 84), {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "# env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "# env.seed(42)\n",
    "# env.action_space.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "# torch.random.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNSolver(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        ).to(device) \n",
    "        self.target = copy.deepcopy(self.online)\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, action_dim, save_directory):\n",
    "        self.action_dim = action_dim\n",
    "        self.save_directory = save_directory\n",
    "        self.net = DDQNSolver(self.action_dim) #.cuda()\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_rate_decay = 0.99\n",
    "        self.exploration_rate_min = 0.01\n",
    "        self.current_step = 0\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.sync_period = 1e4\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025, eps=1e-4)\n",
    "        self.loss = torch.nn.SmoothL1Loss()\n",
    "        self.episode_rewards = []\n",
    "        self.moving_average_episode_rewards = []\n",
    "        self.current_episode_reward = 0.0\n",
    "\n",
    "    def log_episode(self):\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "        self.current_episode_reward = 0.0\n",
    "\n",
    "    def log_period(self, episode, epsilon, step):\n",
    "        self.moving_average_episode_rewards.append(np.round(np.mean(self.episode_rewards[-checkpoint_period:]), 3))\n",
    "        print(f\"Episode {episode} - Step {step} - Epsilon {epsilon} - Mean Reward {self.moving_average_episode_rewards[-1]}\")\n",
    "        plt.plot(self.moving_average_episode_rewards)\n",
    "        plt.savefig(os.path.join(self.save_directory, f\"episode_rewards_plot_{episode}.png\"))\n",
    "        plt.clf()\n",
    "\n",
    "    def remember(self, state, next_state, action, reward, done):\n",
    "        self.memory.append((torch.tensor(state.__array__()), torch.tensor(next_state.__array__()),\n",
    "                            torch.tensor([action]), torch.tensor([reward]), torch.tensor([done])))\n",
    "\n",
    "    def experience_replay(self, step_reward):\n",
    "        self.current_episode_reward += step_reward\n",
    "        if self.current_step % self.sync_period == 0:\n",
    "            self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "        if self.batch_size > len(self.memory):\n",
    "            return\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "        q_estimate = self.net(state, model=\"online\")[np.arange(0, self.batch_size), action]\n",
    "        with torch.no_grad():\n",
    "            best_action = torch.argmax(self.net(next_state, model=\"online\"), dim=1)\n",
    "            next_q = self.net(next_state, model=\"target\")[np.arange(0, self.batch_size), best_action]\n",
    "            q_target = (reward + (1 - done.float()) * self.gamma * next_q).float()\n",
    "        loss = self.loss(q_estimate, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def recall(self):\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*random.sample(self.memory, self.batch_size)))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            action_values = self.net(torch.tensor(state.__array__()).unsqueeze(0).to(device), model=\"online\") #.cuda()\n",
    "            action = torch.argmax(action_values, dim=1).item()\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        self.current_step += 1\n",
    "        return action\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.net.load_state_dict(checkpoint['model'])\n",
    "        self.exploration_rate = checkpoint['exploration_rate']\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        filename = os.path.join(self.save_directory, 'checkpoint_{}.pth'.format(episode))\n",
    "        torch.save(dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_period = 10\n",
    "save_directory = \"mario_ql\"\n",
    "load_checkpoint = None\n",
    "agent = DDQNAgent(action_dim=env.action_space.n, save_directory=save_directory)\n",
    "if load_checkpoint is not None:\n",
    "    agent.load_checkpoint(save_directory + \"/\" + load_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 - Step 870 - Epsilon 0.01 - Mean Reward 392.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_10.pth'\n",
      "Episode 20 - Step 1572 - Epsilon 0.01 - Mean Reward 328.4\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_20.pth'\n",
      "Episode 30 - Step 2396 - Epsilon 0.01 - Mean Reward 421.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_30.pth'\n",
      "Episode 40 - Step 2974 - Epsilon 0.01 - Mean Reward 330.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_40.pth'\n",
      "Episode 50 - Step 3782 - Epsilon 0.01 - Mean Reward 441.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_50.pth'\n",
      "Episode 60 - Step 4494 - Epsilon 0.01 - Mean Reward 419.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_60.pth'\n",
      "Episode 70 - Step 5035 - Epsilon 0.01 - Mean Reward 314.3\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_70.pth'\n",
      "Episode 80 - Step 5677 - Epsilon 0.01 - Mean Reward 358.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_80.pth'\n",
      "Episode 90 - Step 6109 - Epsilon 0.01 - Mean Reward 276.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_90.pth'\n",
      "Episode 100 - Step 7041 - Epsilon 0.01 - Mean Reward 433.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_100.pth'\n",
      "Episode 110 - Step 7684 - Epsilon 0.01 - Mean Reward 368.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_110.pth'\n",
      "Episode 120 - Step 8589 - Epsilon 0.01 - Mean Reward 434.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_120.pth'\n",
      "Episode 130 - Step 9429 - Epsilon 0.01 - Mean Reward 473.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_130.pth'\n",
      "Episode 140 - Step 9950 - Epsilon 0.01 - Mean Reward 271.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_140.pth'\n",
      "Episode 150 - Step 10751 - Epsilon 0.01 - Mean Reward 426.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_150.pth'\n",
      "Episode 160 - Step 12129 - Epsilon 0.01 - Mean Reward 498.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_160.pth'\n",
      "Episode 170 - Step 13147 - Epsilon 0.01 - Mean Reward 546.4\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_170.pth'\n",
      "Episode 180 - Step 14783 - Epsilon 0.01 - Mean Reward 676.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_180.pth'\n",
      "Episode 190 - Step 16755 - Epsilon 0.01 - Mean Reward 788.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_190.pth'\n",
      "Episode 200 - Step 18520 - Epsilon 0.01 - Mean Reward 709.4\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_200.pth'\n",
      "Episode 210 - Step 19848 - Epsilon 0.01 - Mean Reward 703.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_210.pth'\n",
      "Episode 220 - Step 21577 - Epsilon 0.01 - Mean Reward 865.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_220.pth'\n",
      "Episode 230 - Step 23249 - Epsilon 0.01 - Mean Reward 867.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_230.pth'\n",
      "Episode 240 - Step 25093 - Epsilon 0.01 - Mean Reward 980.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_240.pth'\n",
      "Episode 250 - Step 27036 - Epsilon 0.01 - Mean Reward 1088.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_250.pth'\n",
      "Episode 260 - Step 28304 - Epsilon 0.01 - Mean Reward 792.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_260.pth'\n",
      "Episode 270 - Step 29294 - Epsilon 0.01 - Mean Reward 649.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_270.pth'\n",
      "Episode 280 - Step 30730 - Epsilon 0.01 - Mean Reward 932.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_280.pth'\n",
      "Episode 290 - Step 32041 - Epsilon 0.01 - Mean Reward 846.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_290.pth'\n",
      "Episode 300 - Step 33535 - Epsilon 0.01 - Mean Reward 1021.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_300.pth'\n",
      "Episode 310 - Step 34953 - Epsilon 0.01 - Mean Reward 880.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_310.pth'\n",
      "Episode 320 - Step 36563 - Epsilon 0.01 - Mean Reward 1019.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_320.pth'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ddqn.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m next_state, reward, done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m agent\u001b[39m.\u001b[39mremember(state, next_state, action, reward, done)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m agent\u001b[39m.\u001b[39;49mexperience_replay(reward)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ddqn.ipynb Cell 18\u001b[0m in \u001b[0;36mDDQNAgent.experience_replay\u001b[1;34m(self, step_reward)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(q_estimate, q_target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddqn.ipynb#X23sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "episode = 0\n",
    "while True:\n",
    "    state,info = env.reset()\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        env.render()\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        agent.remember(state, next_state, action, reward, done)\n",
    "        agent.experience_replay(reward)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            episode += 1\n",
    "            agent.log_episode()\n",
    "            if episode % checkpoint_period == 0:\n",
    "                agent.log_period(episode=episode, epsilon=agent.exploration_rate, step=agent.current_step)\n",
    "                agent.save_checkpoint()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
