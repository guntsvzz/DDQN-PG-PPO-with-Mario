{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mario with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import random, datetime, os, copy\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 7\n"
     ]
    }
   ],
   "source": [
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Action :\",n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240, 256, 3), {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gym is worked or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# done = True\n",
    "# for step in range(500):\n",
    "#     if done:\n",
    "#         next_state = env.reset()\n",
    "#     next_state, reward, done,_, info = env.step(env.action_space.sample())\n",
    "#     env.render()\n",
    "#     # img=env.render(mode=\"rgb_array\")\n",
    "    \n",
    "# env.close()\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grey Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v1 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((240, 256, 3), {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v1\", render_mode='human', apply_api_compatibility=True)\n",
    "# env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "n_actions = env.action_space.n\n",
    "print(\"Action :\",n_actions)\n",
    "\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 84, 84), {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "# env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "# env.seed(42)\n",
    "# env.action_space.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.action_space.n)\n",
    "        ) #value\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        ) #policy\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return Categorical(logits=self.actor(obs)), self.critic(obs).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOSolver:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.gamma = 0.95\n",
    "        self.lamda = 0.95\n",
    "        self.worker_steps = 4096\n",
    "        self.n_mini_batch = 4\n",
    "        self.epochs = 30\n",
    "        self.save_directory = \"./mario_ppo\"\n",
    "        self.batch_size = self.worker_steps\n",
    "        self.mini_batch_size = self.batch_size // self.n_mini_batch\n",
    "        self.obs,_ = env.reset() #.__array__()\n",
    "        self.obs = self.obs.__array__()\n",
    "        self.policy = Model().to(device)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': 0.00025},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': 0.001}\n",
    "        ], eps=1e-4)\n",
    "        self.policy_old = Model().to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.all_episode_rewards = []\n",
    "        self.all_mean_rewards = []\n",
    "        self.episode = 0\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        filename = os.path.join(self.save_directory, 'checkpoint_{}.pth'.format(self.episode))\n",
    "        torch.save(self.policy_old.state_dict(), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        self.policy.load_state_dict(torch.load(os.path.join(self.save_directory, filename), map_location=torch.device('cpu')))\n",
    "        self.policy_old.load_state_dict(torch.load(os.path.join(self.save_directory, filename), map_location=torch.device('cpu')))\n",
    "        print('Resuming training from checkpoint \\'{}\\'.'.format(filename))\n",
    "\n",
    "    def sample(self):\n",
    "        rewards = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        actions = np.zeros(self.worker_steps, dtype=np.int32)\n",
    "        done = np.zeros(self.worker_steps, dtype=bool)\n",
    "        obs = np.zeros((self.worker_steps, 4, 84, 84), dtype=np.float32)\n",
    "        log_pis = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        values = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        for t in range(self.worker_steps):\n",
    "            with torch.no_grad():\n",
    "                obs[t] = self.obs\n",
    "                pi, v = self.policy_old(torch.tensor(self.obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "                values[t] = v.to(device).numpy()\n",
    "                a = pi.sample()\n",
    "                actions[t] = a.to(device).numpy()\n",
    "                log_pis[t] = pi.log_prob(a).to(device).numpy()\n",
    "            self.obs, rewards[t], done[t], _, info = env.step(actions[t])\n",
    "            self.obs = self.obs.__array__()\n",
    "            env.render()\n",
    "            self.rewards.append(rewards[t])\n",
    "            if done[t]:\n",
    "                print(\"Done\",done[t])\n",
    "                self.episode += 1\n",
    "                self.all_episode_rewards.append(np.sum(self.rewards))\n",
    "                self.rewards = []\n",
    "                env.reset()\n",
    "                if self.episode % 10 == 0:\n",
    "                    print('Episode: {}, average reward: {}'.format(self.episode, np.mean(self.all_episode_rewards[-10:])))\n",
    "                    self.all_mean_rewards.append(np.mean(self.all_episode_rewards[-10:]))\n",
    "                    plt.plot(self.all_mean_rewards)\n",
    "                    plt.savefig(\"{}/mean_reward_{}.png\".format(self.save_directory, self.episode))\n",
    "                    plt.clf()\n",
    "                    self.save_checkpoint()\n",
    "        returns, advantages = self.calculate_advantages(done, rewards, values)\n",
    "        return {\n",
    "            'obs': torch.tensor(obs.reshape(obs.shape[0], *obs.shape[1:]), dtype=torch.float32, device=device),\n",
    "            'actions': torch.tensor(actions, device=device),\n",
    "            'values': torch.tensor(values, device=device),\n",
    "            'log_pis': torch.tensor(log_pis, device=device),\n",
    "            'advantages': torch.tensor(advantages, device=device, dtype=torch.float32),\n",
    "            'returns': torch.tensor(returns, device=device, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "    def calculate_advantages(self, done, rewards, values):\n",
    "        _, last_value = self.policy_old(torch.tensor(self.obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "        last_value = last_value.to(device).data.numpy()\n",
    "        values = np.append(values, last_value)\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            mask = 1.0 - done[i]\n",
    "            delta = rewards[i] + self.gamma * values[i + 1] * mask - values[i]\n",
    "            gae = delta + self.gamma * self.lamda * mask * gae\n",
    "            returns.insert(0, gae + values[i])\n",
    "        adv = np.array(returns) - values[:-1]\n",
    "        return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "\n",
    "    def train(self, samples, clip_range):\n",
    "        indexes = torch.randperm(self.batch_size)\n",
    "        for start in range(0, self.batch_size, self.mini_batch_size):\n",
    "            end = start + self.mini_batch_size\n",
    "            mini_batch_indexes = indexes[start: end]\n",
    "            mini_batch = {}\n",
    "            for k, v in samples.items():\n",
    "                mini_batch[k] = v[mini_batch_indexes]\n",
    "            for _ in range(self.epochs):\n",
    "                loss = self.calculate_loss(clip_range=clip_range, samples=mini_batch)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "    def calculate_loss(self, samples, clip_range):\n",
    "        sampled_returns = samples['returns']\n",
    "        sampled_advantages = samples['advantages']\n",
    "        pi, value = self.policy(samples['obs'])\n",
    "        ratio = torch.exp(pi.log_prob(samples['actions']) - samples['log_pis'])\n",
    "        clipped_ratio = ratio.clamp(min=1.0 - clip_range, max=1.0 + clip_range)\n",
    "        policy_reward = torch.min(ratio * sampled_advantages, clipped_ratio * sampled_advantages)\n",
    "        entropy_bonus = pi.entropy()\n",
    "        vf_loss = self.mse_loss(value, sampled_returns)\n",
    "        loss = -policy_reward + 0.5 * vf_loss - 0.01 * entropy_bonus\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint 'checkpoint_460.pth'.\n"
     ]
    }
   ],
   "source": [
    "load_filename = None\n",
    "load_filename = \"checkpoint_460.pth\"\n",
    "save_directory = \"./mario_ppo\"\n",
    "episode = 0\n",
    "solver = PPOSolver()\n",
    "if load_filename is not None:\n",
    "    episode = solver.load_checkpoint(load_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 470, average reward: 989.7999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_470.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 480, average reward: 857.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_480.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 490, average reward: 1237.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_490.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 500, average reward: 1033.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_500.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 510, average reward: 1168.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_510.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 520, average reward: 1168.300048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_520.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 530, average reward: 1332.300048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_530.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 540, average reward: 1323.699951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_540.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 550, average reward: 1475.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_550.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 560, average reward: 1622.800048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_560.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 570, average reward: 1336.199951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_570.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 580, average reward: 1140.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_580.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 590, average reward: 1410.699951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_590.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 600, average reward: 1807.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_600.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 610, average reward: 1720.300048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_610.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 620, average reward: 1639.0999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_620.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 630, average reward: 1738.800048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_630.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 640, average reward: 1761.300048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_640.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 650, average reward: 1194.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_650.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 660, average reward: 1778.800048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_660.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 670, average reward: 1596.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_670.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 680, average reward: 1720.800048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_680.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 690, average reward: 1685.300048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_690.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 700, average reward: 1397.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_700.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 710, average reward: 1748.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_710.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 720, average reward: 1455.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_720.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 730, average reward: 1434.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_730.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 740, average reward: 1871.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_740.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 750, average reward: 2123.199951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_750.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 760, average reward: 1807.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_760.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 770, average reward: 1544.699951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_770.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 780, average reward: 2032.699951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_780.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 790, average reward: 2182.89990234375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_790.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 800, average reward: 2346.10009765625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_800.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 810, average reward: 1687.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_810.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 820, average reward: 1498.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_820.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 830, average reward: 1896.800048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_830.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 840, average reward: 2499.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_840.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 850, average reward: 2204.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_850.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 860, average reward: 2185.60009765625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_860.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 870, average reward: 2325.60009765625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_870.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 880, average reward: 2797.300048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_880.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 890, average reward: 2542.89990234375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_890.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 900, average reward: 2157.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_900.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 910, average reward: 2211.60009765625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_910.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 920, average reward: 2255.800048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_920.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 930, average reward: 2279.10009765625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_930.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 940, average reward: 2258.199951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_940.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 950, average reward: 2072.199951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_950.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 960, average reward: 1684.300048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_960.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 970, average reward: 1641.300048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_970.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 980, average reward: 2064.199951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_980.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 990, average reward: 2187.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_990.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Done True\n",
      "Episode: 1000, average reward: 2325.699951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1000.pth'\n",
      "Done True\n",
      "Done True\n",
      "Done True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ppo.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m solver\u001b[39m.\u001b[39mepisode \u001b[39m=\u001b[39m \u001b[39m460\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     solver\u001b[39m.\u001b[39mtrain(solver\u001b[39m.\u001b[39;49msample(), \u001b[39m0.2\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ppo.ipynb Cell 17\u001b[0m in \u001b[0;36mPPOSolver.sample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     actions[t] \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     log_pis[t] \u001b[39m=\u001b[39m pi\u001b[39m.\u001b[39mlog_prob(a)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs, rewards[t], done[t], _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions[t])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs\u001b[39m.\u001b[39m__array__()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m env\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\gym\\wrappers\\frame_stack.py:173\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m    165\u001b[0m     \u001b[39m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes\u001b[39m.\u001b[39mappend(observation)\n\u001b[0;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(\u001b[39mNone\u001b[39;00m), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\gym\\core.py:384\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m    383\u001b[0m     \u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\gym\\core.py:384\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m    383\u001b[0m     \u001b[39m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ppo.ipynb Cell 17\u001b[0m in \u001b[0;36mSkipFrame.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_skip):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Accumulate reward and repeat the same action\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     obs, reward, done, trunk, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ppo.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\nes_py\\wrappers\\joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mTake a step using the given action.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m# take the step and record the output\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_action_map[action])\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\gym\\wrappers\\compatibility.py:108\u001b[0m, in \u001b[0;36mEnvCompatibility.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    105\u001b[0m obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    110\u001b[0m \u001b[39mreturn\u001b[39;00m convert_to_terminated_truncated_step_api((obs, reward, done, info))\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\gym\\wrappers\\compatibility.py:118\u001b[0m, in \u001b[0;36mEnvCompatibility.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    113\u001b[0m     \u001b[39m\"\"\"Renders the environment.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \n\u001b[0;32m    115\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m        The rendering of the environment, depending on the render mode\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\nes_py\\nes_env.py:386\u001b[0m, in \u001b[0;36mNESEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer \u001b[39m=\u001b[39m ImageViewer(\n\u001b[0;32m    381\u001b[0m             caption\u001b[39m=\u001b[39mcaption,\n\u001b[0;32m    382\u001b[0m             height\u001b[39m=\u001b[39mSCREEN_HEIGHT,\n\u001b[0;32m    383\u001b[0m             width\u001b[39m=\u001b[39mSCREEN_WIDTH,\n\u001b[0;32m    384\u001b[0m         )\n\u001b[0;32m    385\u001b[0m     \u001b[39m# show the screen on the image viewer\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mviewer\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscreen)\n\u001b[0;32m    387\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\nes_py\\_image_viewer.py:148\u001b[0m, in \u001b[0;36mImageViewer.show\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    140\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpyglet\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mImageData(\n\u001b[0;32m    141\u001b[0m     frame\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[0;32m    142\u001b[0m     frame\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     pitch\u001b[39m=\u001b[39mframe\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m\n\u001b[0;32m    146\u001b[0m )\n\u001b[0;32m    147\u001b[0m \u001b[39m# send the image to the window\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m image\u001b[39m.\u001b[39;49mblit(\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, width\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_window\u001b[39m.\u001b[39;49mwidth, height\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_window\u001b[39m.\u001b[39;49mheight)\n\u001b[0;32m    149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_window\u001b[39m.\u001b[39mflip()\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\pyglet\\image\\__init__.py:904\u001b[0m, in \u001b[0;36mImageData.blit\u001b[1;34m(self, x, y, z, width, height)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mblit\u001b[39m(\u001b[39mself\u001b[39m, x, y, z\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, height\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 904\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_texture()\u001b[39m.\u001b[39mblit(x, y, z, width, height)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\pyglet\\image\\__init__.py:835\u001b[0m, in \u001b[0;36mImageData.get_texture\u001b[1;34m(self, rectangle, force_rectangle)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_texture\u001b[39m(\u001b[39mself\u001b[39m, rectangle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, force_rectangle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    833\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_texture \u001b[39mor\u001b[39;00m\n\u001b[0;32m    834\u001b[0m             (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_texture\u001b[39m.\u001b[39m_is_rectangle \u001b[39mand\u001b[39;00m force_rectangle)):\n\u001b[1;32m--> 835\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_texture \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_texture(Texture, rectangle, force_rectangle)\n\u001b[0;32m    836\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_texture\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\pyglet\\image\\__init__.py:821\u001b[0m, in \u001b[0;36mImageData.create_texture\u001b[1;34m(self, cls, rectangle, force_rectangle)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[39m\"\"\"Create a texture containing this image.\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \n\u001b[0;32m    800\u001b[0m \u001b[39mIf the image's dimensions are not powers of 2, a TextureRegion of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[39m:rtype: cls or cls.region_class\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    820\u001b[0m internalformat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_internalformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat)\n\u001b[1;32m--> 821\u001b[0m texture \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwidth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheight, internalformat,\n\u001b[0;32m    822\u001b[0m                      rectangle, force_rectangle)\n\u001b[0;32m    823\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manchor_x \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manchor_y:\n\u001b[0;32m    824\u001b[0m     texture\u001b[39m.\u001b[39manchor_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manchor_x\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\pyglet\\image\\__init__.py:1488\u001b[0m, in \u001b[0;36mTexture.create\u001b[1;34m(cls, width, height, internalformat, rectangle, force_rectangle, min_filter, mag_filter)\u001b[0m\n\u001b[0;32m   1482\u001b[0m     texture\u001b[39m.\u001b[39m_is_rectangle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m     texture\u001b[39m.\u001b[39mtex_coords \u001b[39m=\u001b[39m (\u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m,\n\u001b[0;32m   1484\u001b[0m                           width, \u001b[39m0.\u001b[39m, \u001b[39m0.\u001b[39m,\n\u001b[0;32m   1485\u001b[0m                           width, height, \u001b[39m0.\u001b[39m,\n\u001b[0;32m   1486\u001b[0m                           \u001b[39m0.\u001b[39m, height, \u001b[39m0.\u001b[39m)\n\u001b[1;32m-> 1488\u001b[0m glFlush()\n\u001b[0;32m   1490\u001b[0m \u001b[39mif\u001b[39;00m texture_width \u001b[39m==\u001b[39m width \u001b[39mand\u001b[39;00m texture_height \u001b[39m==\u001b[39m height:\n\u001b[0;32m   1491\u001b[0m     \u001b[39mreturn\u001b[39;00m texture\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\pyglet\\gl\\lib.py:87\u001b[0m, in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mGLException\u001b[39;00m(\u001b[39mException\u001b[39;00m):\n\u001b[0;32m     84\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merrcheck\u001b[39m(result, func, arguments):\n\u001b[0;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m _debug_gl_trace:\n\u001b[0;32m     89\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver.episode = 460\n",
    "while True:\n",
    "    solver.train(solver.sample(), 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
