{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mario with Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import random, datetime, os, copy\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v1 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 7\n"
     ]
    }
   ],
   "source": [
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Action :\",n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240, 256, 3), {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gym is worked or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "done = True\n",
    "for step in range(500):\n",
    "    if done:\n",
    "        next_state = env.reset()\n",
    "    next_state, reward, done,_, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    # img=env.render(mode=\"rgb_array\")\n",
    "    \n",
    "env.close()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grey Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((240, 256, 3), {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v1\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Action :\",n_actions)\n",
    "\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 84, 84), {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "# env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "# env.seed(42)\n",
    "# env.action_space.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioSolver:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.action_space.n),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ).to(device) #.cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, eps=1e-4)\n",
    "        self.reset()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def reset(self):\n",
    "        self.episode_actions = torch.tensor([], requires_grad=True).to(device) #.cuda()\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def save_checkpoint(self, directory, episode):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        filename = os.path.join(directory, 'checkpoint_{}.pth'.format(episode))\n",
    "        torch.save(self.model.state_dict(), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
    "\n",
    "    def load_checkpoint(self, directory, filename):\n",
    "        self.model.load_state_dict(torch.load(os.path.join(directory, filename)))\n",
    "        print('Resuming training from checkpoint \\'{}\\'.'.format(filename))\n",
    "        return int(filename[11:-4])\n",
    "\n",
    "    def backward(self):\n",
    "        future_reward = 0\n",
    "        rewards = []\n",
    "        for r in self.episode_rewards[::-1]:\n",
    "            future_reward = r + gamma * future_reward\n",
    "            rewards.append(future_reward)\n",
    "        rewards = torch.tensor(rewards[::-1], dtype=torch.float32).to(device) #.cuda()\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "        loss = torch.sum(torch.mul(self.episode_actions, rewards).mul(-1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from checkpoint 'checkpoint_3000.pth'.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "gamma = 0.95\n",
    "load_filename = None\n",
    "load_filename = \"checkpoint_3000.pth\"\n",
    "save_directory = \"./mario_pg\"\n",
    "batch_rewards = []\n",
    "episode = 0\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "model = MarioSolver(learning_rate=0.00025)\n",
    "if load_filename is not None:\n",
    "    episode = model.load_checkpoint(save_directory, load_filename)\n",
    "all_episode_rewards = []\n",
    "all_mean_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 301, average reward: 1384.7\n",
      "Batch: 302, average reward: 918.3\n",
      "Batch: 303, average reward: 1284.5\n",
      "Batch: 304, average reward: 1005.0\n",
      "Batch: 305, average reward: 1331.3\n",
      "Batch: 306, average reward: 1378.3\n",
      "Batch: 307, average reward: 1681.2\n",
      "Batch: 308, average reward: 1574.5\n",
      "Batch: 309, average reward: 1105.5\n",
      "Batch: 310, average reward: 1282.3\n",
      "Batch: 311, average reward: 1373.8\n",
      "Batch: 312, average reward: 1436.2\n",
      "Batch: 313, average reward: 2020.8\n",
      "Batch: 314, average reward: 1851.7\n",
      "Batch: 315, average reward: 1964.7\n",
      "Batch: 316, average reward: 1482.6\n",
      "Batch: 317, average reward: 1373.8\n",
      "Batch: 318, average reward: 1388.6\n",
      "Batch: 319, average reward: 1491.1\n",
      "Batch: 320, average reward: 1838.5\n",
      "Batch: 321, average reward: 1737.9\n",
      "Batch: 322, average reward: 1879.1\n",
      "Batch: 323, average reward: 2030.5\n",
      "Batch: 324, average reward: 1517.0\n",
      "Batch: 325, average reward: 1587.8\n",
      "Batch: 326, average reward: 1372.3\n",
      "Batch: 327, average reward: 1845.0\n",
      "Batch: 328, average reward: 1818.3\n",
      "Batch: 329, average reward: 1782.8\n",
      "Batch: 330, average reward: 1972.8\n",
      "Batch: 331, average reward: 2071.5\n",
      "Batch: 332, average reward: 1770.9\n",
      "Batch: 333, average reward: 2097.2\n",
      "Batch: 334, average reward: 2246.7\n",
      "Batch: 335, average reward: 2141.3\n",
      "Batch: 336, average reward: 1629.1\n",
      "Batch: 337, average reward: 1562.1\n",
      "Batch: 338, average reward: 2207.7\n",
      "Batch: 339, average reward: 2129.9\n",
      "Batch: 340, average reward: 2309.4\n",
      "Batch: 341, average reward: 2232.1\n",
      "Batch: 342, average reward: 2170.2\n",
      "Batch: 343, average reward: 1513.6\n",
      "Batch: 344, average reward: 1617.6\n",
      "Batch: 345, average reward: 1128.5\n",
      "Batch: 346, average reward: 1785.4\n",
      "Batch: 347, average reward: 2130.7\n",
      "Batch: 348, average reward: 2176.5\n",
      "Batch: 349, average reward: 1781.0\n",
      "Batch: 350, average reward: 1467.5\n",
      "Checkpoint saved to './mario_pg\\checkpoint_3500.pth'\n",
      "Batch: 351, average reward: 1707.9\n",
      "Batch: 352, average reward: 2031.1\n",
      "Batch: 353, average reward: 2139.1\n",
      "Batch: 354, average reward: 1979.4\n",
      "Batch: 355, average reward: 2056.0\n",
      "Batch: 356, average reward: 1913.4\n",
      "Batch: 357, average reward: 2308.4\n",
      "Batch: 358, average reward: 2296.9\n",
      "Batch: 359, average reward: 2095.6\n",
      "Batch: 360, average reward: 2420.3\n",
      "Batch: 361, average reward: 2391.3\n",
      "Batch: 362, average reward: 2199.8\n",
      "Batch: 363, average reward: 2224.0\n",
      "Batch: 364, average reward: 2193.2\n",
      "Batch: 365, average reward: 2572.9\n",
      "Batch: 366, average reward: 869.6\n",
      "Batch: 367, average reward: 1395.8\n",
      "Batch: 368, average reward: 1471.9\n",
      "Batch: 369, average reward: 2277.6\n",
      "Batch: 370, average reward: 2194.5\n",
      "Batch: 371, average reward: 2493.2\n",
      "Batch: 372, average reward: 2715.5\n",
      "Batch: 373, average reward: 2544.7\n",
      "Batch: 374, average reward: 2251.6\n",
      "Batch: 375, average reward: 2306.4\n",
      "Batch: 376, average reward: 2757.4\n",
      "Batch: 377, average reward: 2526.0\n",
      "Batch: 378, average reward: 2038.6\n",
      "Batch: 379, average reward: 1988.7\n",
      "Batch: 380, average reward: 1859.9\n",
      "Batch: 381, average reward: 1834.5\n",
      "Batch: 382, average reward: 2194.8\n",
      "Batch: 383, average reward: 1643.2\n",
      "Batch: 384, average reward: 2032.7\n",
      "Batch: 385, average reward: 2088.6\n",
      "Batch: 386, average reward: 2357.9\n",
      "Batch: 387, average reward: 2390.5\n",
      "Batch: 388, average reward: 2182.0\n",
      "Batch: 389, average reward: 1859.6\n",
      "Batch: 390, average reward: 1980.6\n",
      "Batch: 391, average reward: 2332.1\n",
      "Batch: 392, average reward: 2213.1\n",
      "Batch: 393, average reward: 1821.6\n",
      "Batch: 394, average reward: 2055.3\n",
      "Batch: 395, average reward: 2152.2\n",
      "Batch: 396, average reward: 2459.5\n",
      "Batch: 397, average reward: 1916.6\n",
      "Batch: 398, average reward: 1727.7\n",
      "Batch: 399, average reward: 2049.1\n",
      "Batch: 400, average reward: 2192.9\n",
      "Checkpoint saved to './mario_pg\\checkpoint_4000.pth'\n",
      "Batch: 401, average reward: 1834.5\n",
      "Batch: 402, average reward: 1950.9\n",
      "Batch: 403, average reward: 2546.9\n",
      "Batch: 404, average reward: 2454.9\n",
      "Batch: 405, average reward: 2443.2\n",
      "Batch: 406, average reward: 2283.4\n",
      "Batch: 407, average reward: 2415.7\n",
      "Batch: 408, average reward: 2759.7\n",
      "Batch: 409, average reward: 2626.6\n",
      "Batch: 410, average reward: 2185.4\n",
      "Batch: 411, average reward: 2529.9\n",
      "Batch: 412, average reward: 2462.0\n",
      "Batch: 413, average reward: 2662.7\n",
      "Batch: 414, average reward: 2826.0\n",
      "Batch: 415, average reward: 2705.5\n",
      "Batch: 416, average reward: 2379.0\n",
      "Batch: 417, average reward: 2668.6\n",
      "Batch: 418, average reward: 2687.6\n",
      "Batch: 419, average reward: 2153.7\n",
      "Batch: 420, average reward: 2936.1\n",
      "Batch: 421, average reward: 2908.7\n",
      "Batch: 422, average reward: 2723.7\n",
      "Batch: 423, average reward: 2772.5\n",
      "Batch: 424, average reward: 2980.0\n",
      "Batch: 425, average reward: 2707.2\n",
      "Batch: 426, average reward: 2496.1\n",
      "Batch: 427, average reward: 2933.6\n",
      "Batch: 428, average reward: 2443.3\n",
      "Batch: 429, average reward: 1856.9\n",
      "Batch: 430, average reward: 2571.1\n",
      "Batch: 431, average reward: 2220.2\n",
      "Batch: 432, average reward: 1204.6\n",
      "Batch: 433, average reward: 2070.9\n",
      "Batch: 434, average reward: 2547.4\n",
      "Batch: 435, average reward: 2483.2\n",
      "Batch: 436, average reward: 2031.5\n",
      "Batch: 437, average reward: 2485.7\n",
      "Batch: 438, average reward: 2504.4\n",
      "Batch: 439, average reward: 2799.3\n",
      "Batch: 440, average reward: 2608.3\n",
      "Batch: 441, average reward: 2828.4\n",
      "Batch: 442, average reward: 2006.3\n",
      "Batch: 443, average reward: 2230.0\n",
      "Batch: 444, average reward: 2566.0\n",
      "Batch: 445, average reward: 2646.2\n",
      "Batch: 446, average reward: 2646.3\n",
      "Batch: 447, average reward: 2624.2\n",
      "Batch: 448, average reward: 2604.7\n",
      "Batch: 449, average reward: 2229.4\n",
      "Batch: 450, average reward: 2433.3\n",
      "Checkpoint saved to './mario_pg\\checkpoint_4500.pth'\n",
      "Batch: 451, average reward: 2112.7\n",
      "Batch: 452, average reward: 2257.6\n",
      "Batch: 453, average reward: 2827.0\n",
      "Batch: 454, average reward: 2869.2\n",
      "Batch: 455, average reward: 2600.4\n",
      "Batch: 456, average reward: 2535.3\n",
      "Batch: 457, average reward: 2837.4\n",
      "Batch: 458, average reward: 2332.5\n",
      "Batch: 459, average reward: 2978.2\n",
      "Batch: 460, average reward: 2331.2\n",
      "Batch: 461, average reward: 2661.7\n",
      "Batch: 462, average reward: 2401.8\n",
      "Batch: 463, average reward: 2522.4\n",
      "Batch: 464, average reward: 2697.3\n",
      "Batch: 465, average reward: 2936.3\n",
      "Batch: 466, average reward: 2768.5\n",
      "Batch: 467, average reward: 2805.9\n",
      "Batch: 468, average reward: 2353.3\n",
      "Batch: 469, average reward: 2502.5\n",
      "Batch: 470, average reward: 2845.3\n",
      "Batch: 471, average reward: 2469.2\n",
      "Batch: 472, average reward: 2735.3\n",
      "Batch: 473, average reward: 2777.8\n",
      "Batch: 474, average reward: 2845.0\n",
      "Batch: 475, average reward: 2464.8\n",
      "Batch: 476, average reward: 2592.0\n",
      "Batch: 477, average reward: 2644.4\n",
      "Batch: 478, average reward: 2800.3\n",
      "Batch: 479, average reward: 2756.2\n",
      "Batch: 480, average reward: 2668.6\n",
      "Batch: 481, average reward: 2913.2\n",
      "Batch: 482, average reward: 2592.5\n",
      "Batch: 483, average reward: 2594.5\n",
      "Batch: 484, average reward: 2524.2\n",
      "Batch: 485, average reward: 2703.7\n",
      "Batch: 486, average reward: 2737.7\n",
      "Batch: 487, average reward: 2525.2\n",
      "Batch: 488, average reward: 2869.6\n",
      "Batch: 489, average reward: 2701.8\n",
      "Batch: 490, average reward: 2594.3\n",
      "Batch: 491, average reward: 2273.9\n",
      "Batch: 492, average reward: 2663.2\n",
      "Batch: 493, average reward: 2403.0\n",
      "Batch: 494, average reward: 2462.8\n",
      "Batch: 495, average reward: 2458.0\n",
      "Batch: 496, average reward: 2591.9\n",
      "Batch: 497, average reward: 2539.1\n",
      "Batch: 498, average reward: 2266.3\n",
      "Batch: 499, average reward: 2697.1\n",
      "Batch: 500, average reward: 2595.8\n",
      "Checkpoint saved to './mario_pg\\checkpoint_5000.pth'\n",
      "Batch: 501, average reward: 2527.3\n",
      "Batch: 502, average reward: 2414.4\n",
      "Batch: 503, average reward: 2475.0\n",
      "Batch: 504, average reward: 2278.5\n",
      "Batch: 505, average reward: 1985.5\n",
      "Batch: 506, average reward: 2452.4\n",
      "Batch: 507, average reward: 2287.5\n",
      "Batch: 508, average reward: 2605.2\n",
      "Batch: 509, average reward: 2029.2\n",
      "Batch: 510, average reward: 2541.5\n",
      "Batch: 511, average reward: 2590.5\n",
      "Batch: 512, average reward: 2684.4\n",
      "Batch: 513, average reward: 2588.2\n",
      "Batch: 514, average reward: 1984.5\n",
      "Batch: 515, average reward: 2468.7\n",
      "Batch: 516, average reward: 2568.5\n",
      "Batch: 517, average reward: 2868.2\n",
      "Batch: 518, average reward: 2910.8\n",
      "Batch: 519, average reward: 2400.5\n",
      "Batch: 520, average reward: 2984.2\n",
      "Batch: 521, average reward: 2860.8\n",
      "Batch: 522, average reward: 3051.8\n",
      "Batch: 523, average reward: 2494.8\n",
      "Batch: 524, average reward: 2199.4\n",
      "Batch: 525, average reward: 2707.1\n",
      "Batch: 526, average reward: 2758.7\n",
      "Batch: 527, average reward: 2799.8\n",
      "Batch: 528, average reward: 2847.4\n",
      "Batch: 529, average reward: 2650.5\n",
      "Batch: 530, average reward: 2864.9\n",
      "Batch: 531, average reward: 2903.6\n",
      "Batch: 532, average reward: 2736.0\n",
      "Batch: 533, average reward: 2602.5\n",
      "Batch: 534, average reward: 2868.4\n",
      "Batch: 535, average reward: 2682.0\n",
      "Batch: 536, average reward: 2936.2\n",
      "Batch: 537, average reward: 2221.3\n",
      "Batch: 538, average reward: 2590.4\n",
      "Batch: 539, average reward: 1862.3\n",
      "Batch: 540, average reward: 2745.2\n",
      "Batch: 541, average reward: 2400.7\n",
      "Batch: 542, average reward: 2708.3\n",
      "Batch: 543, average reward: 2635.2\n",
      "Batch: 544, average reward: 2571.6\n",
      "Batch: 545, average reward: 2421.5\n",
      "Batch: 546, average reward: 2697.0\n",
      "Batch: 547, average reward: 2488.5\n",
      "Batch: 548, average reward: 2395.1\n",
      "Batch: 549, average reward: 2604.4\n",
      "Batch: 550, average reward: 2528.1\n",
      "Checkpoint saved to './mario_pg\\checkpoint_5500.pth'\n",
      "Batch: 551, average reward: 2351.7\n",
      "Batch: 552, average reward: 2690.0\n",
      "Batch: 553, average reward: 2869.3\n",
      "Batch: 554, average reward: 2068.0\n",
      "Batch: 555, average reward: 2020.1\n",
      "Batch: 556, average reward: 2530.7\n",
      "Batch: 557, average reward: 2070.8\n",
      "Batch: 558, average reward: 2005.0\n",
      "Batch: 559, average reward: 1550.4\n",
      "Batch: 560, average reward: 2057.0\n",
      "Batch: 561, average reward: 2349.7\n",
      "Batch: 562, average reward: 2130.3\n",
      "Batch: 563, average reward: 1998.8\n",
      "Batch: 564, average reward: 2658.4\n",
      "Batch: 565, average reward: 2618.0\n",
      "Batch: 566, average reward: 2406.1\n",
      "Batch: 567, average reward: 2821.4\n",
      "Batch: 568, average reward: 2673.7\n",
      "Batch: 569, average reward: 2591.4\n",
      "Batch: 570, average reward: 2860.2\n",
      "Batch: 571, average reward: 2898.2\n",
      "Batch: 572, average reward: 3012.9\n",
      "Batch: 573, average reward: 2697.6\n",
      "Batch: 574, average reward: 2421.2\n",
      "Batch: 575, average reward: 2823.0\n",
      "Batch: 576, average reward: 2704.9\n",
      "Batch: 577, average reward: 2971.5\n",
      "Batch: 578, average reward: 2897.8\n",
      "Batch: 579, average reward: 3040.2\n",
      "Batch: 580, average reward: 2843.9\n",
      "Batch: 581, average reward: 2808.1\n",
      "Batch: 582, average reward: 3007.5\n",
      "Batch: 583, average reward: 3051.8\n",
      "Batch: 584, average reward: 2970.9\n",
      "Batch: 585, average reward: 2890.0\n",
      "Batch: 586, average reward: 3048.8\n",
      "Batch: 587, average reward: 2840.4\n",
      "Batch: 588, average reward: 2563.6\n",
      "Batch: 589, average reward: 2770.4\n",
      "Batch: 590, average reward: 3046.5\n",
      "Batch: 591, average reward: 3044.9\n",
      "Batch: 592, average reward: 2810.4\n",
      "Batch: 593, average reward: 2937.5\n",
      "Batch: 594, average reward: 2677.8\n",
      "Batch: 595, average reward: 3041.3\n",
      "Batch: 596, average reward: 3053.4\n",
      "Batch: 597, average reward: 2924.4\n",
      "Batch: 598, average reward: 3046.3\n",
      "Batch: 599, average reward: 2766.4\n",
      "Batch: 600, average reward: 3012.3\n",
      "Checkpoint saved to './mario_pg\\checkpoint_6000.pth'\n",
      "Batch: 601, average reward: 2807.3\n",
      "Batch: 602, average reward: 2802.0\n",
      "Batch: 603, average reward: 2810.6\n",
      "Batch: 604, average reward: 3047.0\n",
      "Batch: 605, average reward: 2736.5\n",
      "Batch: 606, average reward: 2958.9\n",
      "Batch: 607, average reward: 2791.0\n",
      "Batch: 608, average reward: 2848.6\n",
      "Batch: 609, average reward: 2717.4\n",
      "Batch: 610, average reward: 2942.1\n",
      "Batch: 611, average reward: 3024.8\n",
      "Batch: 612, average reward: 2975.9\n",
      "Batch: 613, average reward: 2891.3\n",
      "Batch: 614, average reward: 2969.4\n",
      "Batch: 615, average reward: 2807.4\n",
      "Batch: 616, average reward: 2715.2\n",
      "Batch: 617, average reward: 3017.7\n",
      "Batch: 618, average reward: 2996.3\n",
      "Batch: 619, average reward: 2870.9\n",
      "Batch: 620, average reward: 2711.5\n",
      "Batch: 621, average reward: 2595.0\n",
      "Batch: 622, average reward: 2535.3\n",
      "Batch: 623, average reward: 2308.4\n",
      "Batch: 624, average reward: 606.0\n",
      "Batch: 625, average reward: 570.6\n",
      "Batch: 626, average reward: 606.0\n",
      "Batch: 627, average reward: 815.3\n",
      "Batch: 628, average reward: 669.9\n",
      "Batch: 629, average reward: 2456.0\n",
      "Batch: 630, average reward: 2338.8\n",
      "Batch: 631, average reward: 2250.9\n",
      "Batch: 632, average reward: 2412.1\n",
      "Batch: 633, average reward: 2134.3\n",
      "Batch: 634, average reward: 2558.0\n",
      "Batch: 635, average reward: 2372.9\n",
      "Batch: 636, average reward: 2838.5\n",
      "Batch: 637, average reward: 2399.7\n",
      "Batch: 638, average reward: 1957.0\n",
      "Batch: 639, average reward: 2654.0\n",
      "Batch: 640, average reward: 2012.4\n",
      "Batch: 641, average reward: 2107.9\n",
      "Batch: 642, average reward: 2718.9\n",
      "Batch: 643, average reward: 2234.5\n",
      "Batch: 644, average reward: 2285.9\n",
      "Batch: 645, average reward: 2698.4\n",
      "Batch: 646, average reward: 2403.6\n",
      "Batch: 647, average reward: 2341.4\n",
      "Batch: 648, average reward: 2541.4\n",
      "Batch: 649, average reward: 2724.0\n",
      "Batch: 650, average reward: 2421.9\n",
      "Checkpoint saved to './mario_pg\\checkpoint_6500.pth'\n",
      "Batch: 651, average reward: 2611.7\n",
      "Batch: 652, average reward: 2326.3\n",
      "Batch: 653, average reward: 2088.0\n",
      "Batch: 654, average reward: 2155.0\n",
      "Batch: 655, average reward: 2555.2\n",
      "Batch: 656, average reward: 2760.3\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    observation,info  = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        # observation = torch.tensor(observation.__array__()).cuda().unsqueeze(0)\n",
    "        observation = torch.tensor(observation.__array__()).unsqueeze(0).to(device)\n",
    "        distribution = Categorical(model.forward(observation))\n",
    "        action = distribution.sample()\n",
    "        observation, reward, done, _, info = env.step(action.item())\n",
    "        model.episode_actions = torch.cat([model.episode_actions, distribution.log_prob(action).reshape(1)])\n",
    "        model.episode_rewards.append(reward)\n",
    "        if done:\n",
    "            all_episode_rewards.append(np.sum(model.episode_rewards))\n",
    "            batch_rewards.append(np.sum(model.episode_rewards))\n",
    "            model.backward()\n",
    "            episode += 1\n",
    "            if episode % batch_size == 0:\n",
    "                print('Batch: {}, average reward: {}'.format(episode // batch_size, np.array(batch_rewards).mean()))\n",
    "                batch_rewards = []\n",
    "                all_mean_rewards.append(np.mean(all_episode_rewards[-batch_size:]))\n",
    "                plt.plot(all_mean_rewards)\n",
    "                if episode % 500 == 0:\n",
    "                    plt.savefig(\"{}/mean_reward_{}.png\".format(save_directory, episode))\n",
    "                plt.clf()\n",
    "            if episode % 500 == 0 and save_directory is not None:\n",
    "                model.save_checkpoint(save_directory, episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
